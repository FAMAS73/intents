{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXnmhU-6SCsC"
      },
      "source": [
        "Data preprocess:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlQJjdGj5Ynm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "with open(\"preprocessed_dataset.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "df = pd.DataFrame(data['intents'])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7aYDD0tRFRac"
      },
      "outputs": [],
      "source": [
        "# Extract patterns and intents\n",
        "patterns = []\n",
        "intents = []\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        patterns.append(pattern)\n",
        "        intents.append(intent[\"tag\"])\n",
        "\n",
        "# Step 2: Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(patterns)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_seq_length = max([len(seq.split()) for seq in patterns])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(patterns)\n",
        "X = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Convert intents to numerical labels\n",
        "label2idx = {label: idx for idx, label in enumerate(np.unique(intents))}\n",
        "y = np.array([label2idx[label] for label in intents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbB6FRTNSKIi"
      },
      "source": [
        "build and train model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWOzeZee5t8H"
      },
      "outputs": [],
      "source": [
        "# Step 3: Build the model architecture with TensorFlow Lite-compatible operations\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(max_seq_length,)))\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, mask_zero=True))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(LayerNormalization())\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(LayerNormalization())\n",
        "model.add(LSTM(32))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(np.unique(y)), activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWYPOLsF5xmQ"
      },
      "outputs": [],
      "source": [
        "# Step 4: Compile the model\n",
        "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "history = model.fit(X, y, epochs=50, batch_size=16, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_lV8ANQdHSq"
      },
      "source": [
        "evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HFYVgMTRxsj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(\"Validation Loss:\", loss)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChVIIQKdJ53N"
      },
      "source": [
        "save model and tokenizer for later use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bKjpxPxq6Zs7"
      },
      "outputs": [],
      "source": [
        "# Step 6: Save the trained model\n",
        "model.save(\"intent_model.h5\")\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer.to_json())\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "    for label, idx in label2idx.items():\n",
        "        f.write(f\"{idx}: {label}\\n\")\n",
        "\n",
        "# Save the word index to JSON\n",
        "with open('word_index.json', 'w') as f:\n",
        "    json.dump(tokenizer.word_index, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9XV_TqiSYho"
      },
      "source": [
        "save to tflite for flutter deployment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MIeOYMZ-6v3W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 8: Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter._experimental_lower_tensor_list_ops = False  # Disable lowering tensor list ops\n",
        "tflite_model = converter.convert()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aor-8Flv_0xx"
      },
      "outputs": [],
      "source": [
        "# Save the TFLite model to a file\n",
        "with open('intent_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4BmOzvLa7I"
      },
      "source": [
        "Test the model with responsive chat and test translation to thai:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvtRINrNAPxg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 9: Test the TensorFlow Lite model with a responsive chat\n",
        "def predict_intent_tflite(text):\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_seq_length, padding='post').astype(np.float32)  # Convert to FLOAT32\n",
        "    interpreter.set_tensor(input_details[0]['index'], padded_sequence)\n",
        "    interpreter.invoke()\n",
        "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predicted_label = np.argmax(prediction)\n",
        "    for label, idx in label2idx.items():\n",
        "        if idx == predicted_label:\n",
        "            return label\n",
        "\n",
        "# Test the TensorFlow Lite model\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    intent = predict_intent_tflite(user_input)\n",
        "    responses = [response[\"responses\"] for response in data[\"intents\"] if response[\"tag\"] == intent]\n",
        "    if responses:\n",
        "        print(\"Bot:\", np.random.choice(responses[0]))\n",
        "    else:\n",
        "        print(\"Bot: Sorry, I didn't understand that.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seiz76AtPNvv"
      },
      "source": [
        "thai:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujM5QWrYN3m-"
      },
      "outputs": [],
      "source": [
        "!%pip install googletrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nrwZRPdN2D9"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator, LANGUAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chj_DGUUNq2y"
      },
      "outputs": [],
      "source": [
        "# Step 9: Test the TensorFlow Lite model with a responsive chat\n",
        "translator = Translator()\n",
        "\n",
        "def translate_to_english(text):\n",
        "    try:\n",
        "        translated_text = translator.translate(text, src='th', dest='en').text\n",
        "        return translated_text\n",
        "    except Exception as e:\n",
        "        print(\"Translation Error:\", e)\n",
        "        return text\n",
        "\n",
        "def translate_to_thai(text):\n",
        "    try:\n",
        "        translated_text = translator.translate(text, src='en', dest='th').text\n",
        "        return translated_text\n",
        "    except Exception as e:\n",
        "        print(\"Translation Error:\", e)\n",
        "        return text\n",
        "\n",
        "def predict_intent_tflite(text):\n",
        "    text = translate_to_english(text)  # Translate input to English\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_seq_length, padding='post')\n",
        "    interpreter.set_tensor(input_details[0]['index'], padded_sequence)\n",
        "    interpreter.invoke()\n",
        "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predicted_label = np.argmax(prediction)\n",
        "    for label, idx in label2idx.items():\n",
        "        if idx == predicted_label:\n",
        "            return label\n",
        "\n",
        "# Test the TensorFlow Lite model\n",
        "while True:\n",
        "    user_input = input(\"You (Thai): \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    english_input = translate_to_english(user_input)  # Translate input to English\n",
        "    intent = predict_intent_tflite(english_input)\n",
        "    responses = [response[\"responses\"] for response in data[\"intents\"] if response[\"tag\"] == intent]\n",
        "    if responses:\n",
        "        response_text = np.random.choice(responses[0])\n",
        "        thai_response = translate_to_thai(response_text)  # Translate response to Thai\n",
        "        print(\"Bot (Thai):\", thai_response)\n",
        "    else:\n",
        "        print(\"Bot (Thai): Sorry, I didn't understand that.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
